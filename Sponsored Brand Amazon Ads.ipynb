{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Data Cleaning project\n",
    "\n",
    "\n",
    "This Jupyter notebook is part of an automation project for data cleaning in Amazon Ads. Managing two distinct ad accounts, each with its unique set of brands, campaign types, and targetings, poses significant challenges in data processing and management. Initially, the data cleaning process was cumbersome and inefficient, primarily conducted in Excel, which proved to be unreliable and time-consuming.\n",
    "\n",
    "The primary objective of this project is to streamline the data cleaning process, reducing the steps involved to a minimum and significantly cutting down the time required. This notebook demonstrates a sample of the data cleaning procedure applied to various sheets, which previously took 1-2 hours, but can now be completed in a matter of seconds.\n",
    "\n",
    "Along the way I've created multiple functions that will come in handy in my future projects!\n",
    "\n",
    "Future enhancements include the integration of Watchdog, an automation tool, to further simplify the process. The goal is to achieve a system where new files are automatically detected and processed, producing the final output without manual intervention.\n",
    "\n",
    "This code, along with its detailed documentation, will be shared on my GitHub account, offering insights into the methods and reasoning behind this automated data cleaning approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries for the data cleaning process\n",
    "\n",
    "import pandas as pd\n",
    "import unicodedata #  The unicodedata module is imported to handle Arabic characters effectively.\n",
    "import re #used to extract the number of the file\n",
    "import sys\n",
    "import os\n",
    "import concurrent.futures # for more efficient execution of importing files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory with your files\n",
    "path_main_file = 'file path' \n",
    "products_file_path = 'Product file' # products are coded, So I imported a file with the product name - code - brand for better analysis\n",
    "def read_csv_file(file_path, columns=None):\n",
    "    \"\"\"This function imports csv files\"\"\"\n",
    "    if columns:\n",
    "        return pd.read_csv(file_path, usecols=columns)\n",
    "    else:\n",
    "        return pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    SBASIN_main,products_file_main = executor.map(read_csv_file,[path_main_file,products_file_path])#[None,columns_of_interest,products_file_columns]\n",
    "\n",
    "\n",
    "SBASIN_main['Date'] = pd.to_datetime(SBASIN_main['Date']) # For data manuipulation I made sure all dates where in datetime format.\n",
    "\n",
    "SBASIN = SBASIN_main\n",
    "# in a specific report the file names were exactly the same except for the number in the end indicating the version, therefore I extracted this number to differentiate between the old and the new files to only keep the new files. \n",
    "SBASIN['source_file'] = os.path.basename(path_main_file)\n",
    "SBASIN['source_file_n'] = SBASIN['source_file'].apply(lambda x: re.search(r'\\((\\d+)\\)', x).group(1) if re.search(r'\\((\\d+)\\)', x) else None)\n",
    "SBASIN['source_file_number'] = SBASIN['source_file_n'].astype(int)\n",
    "SBASIN['Date'] = pd.to_datetime(SBASIN['Date'])\n",
    "\n",
    "products_file = products_file_main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sponsored_Brand_ASIN(SBASIN):\n",
    "    \"\"\"This is the main function to fully clean and export the file into a ready for analysis CSV\"\"\"\n",
    "\n",
    "    def clean_convert_to_numeric(column):\n",
    "        \"\"\"Because Arabic letters are RTL this posed an interesting challenge as even after removed it left a space that couldn't be deleted, therefore i needed to deal with the RTL characters and remove them\"\"\"\n",
    "        def remove_rtl_characters(text):\n",
    "            \"\"\"Removing RTL characters to process the data correctly\"\"\"\n",
    "            return ''.join(char for char in text if unicodedata.category(char) != 'Lm' and unicodedata.category(char) != 'Cf')\n",
    "        \n",
    "        cleaned_column = column.astype(str).apply(remove_rtl_characters)\n",
    "        cleaned_column = cleaned_column.str.replace('ج.م.', '', regex=False)\n",
    "        cleaned_column = cleaned_column.astype(float)\n",
    "        return cleaned_column\n",
    "\n",
    "    # adding the new columns with the updated numbers\n",
    "    SBASIN['14 Day Total Sales'] = clean_convert_to_numeric(SBASIN['14 Day Total Sales'])\n",
    "    SBASIN['14 Day New-to-brand Sales'] = clean_convert_to_numeric(SBASIN['14 Day Total Sales'])\n",
    "\n",
    "    # Sort by 'source file name' descending, then by date, campaign name, adgroup name, asin\n",
    "\n",
    "    SBASIN.sort_values(by=['Date','Campaign Name','Attribution type','Purchased ASIN'], ascending=[True,True,True,True], inplace=True)\n",
    "\n",
    "    # Merge to get information about each product (Brand - Product name)\n",
    "    cleaned_file = SBASIN.merge(products_file, how='left', left_on='Purchased ASIN', right_on= 'ASIN')\n",
    "    \n",
    "    # Drop columns after merge\n",
    "    cleaned_file.drop(columns=['ASIN','Product Name'], inplace=True)\n",
    "\n",
    "    # Renaming columns for better understanding\n",
    "    cleaned_file = cleaned_file.rename(columns={'Brand Description':'Brand','Category Description':'Category','Purchased ASIN':'ASIN','Viewable impressions':'Vimp','14-day Detail Page Views (DPV)':'DPV','14 Day Total Orders (#)':'Total Orders','14 Day Total Units (#)':'Total Units','14 Day Total Sales':'Total Sales','14 Day New-to-brand Orders (#)':'NTB Orders','14 Day New-to-brand Sales':'NTB Sales','14 Day New-to-brand Units (#)':'NTB Units','14-Day Total Orders (#) \\u2013 (Click)':'Total Orders(click)','14-Day Total Units (#) \\u2013 (Click)':'Total Units(click)','14-Day Total Sales \\u2013 (Click)':'Total Sales(click)','14-Day New-to-brand Orders (#) \\u2013 (Click)':'NTB Orders(click)','14-Day New-to-brand Sales \\u2013 (Click)':'NTB Sales (click)','14-Day New-to-Brand Units (#) \\u2013 (Click)':'NTB Units (click)'})\n",
    "\n",
    "\n",
    "    # fillna\n",
    "    cleaned_file.fillna(\"Not Advertised ASIN\", inplace=True)\n",
    "\n",
    "    #day of week\n",
    "    cleaned_file['Day of week'] = cleaned_file['Date'].dt.dayofweek\n",
    "    # map the day of the week number to its name\n",
    "    cleaned_file['Day of week'] = cleaned_file['Day of week'].map({\n",
    "        0: 'Monday',\n",
    "        1: 'Tuesday',\n",
    "        2: 'Wednesday',\n",
    "        3: 'Thursday',\n",
    "        4: 'Friday',\n",
    "        5: 'Saturday',\n",
    "        6: 'Sunday'\n",
    "    })\n",
    "\n",
    "    #Re-arranging columns\n",
    "    cleaned_file = cleaned_file.reindex(columns=['Date',\n",
    "    'Day of week',\n",
    "    'Campaign Name',\n",
    "    'Attribution type',\n",
    "    'ASIN',\n",
    "    'Brand',\n",
    "    'Product',\n",
    "    'Category',\n",
    "    'Total Orders',\n",
    "    'Total Units',\n",
    "    'Total Sales',\n",
    "    'NTB Orders',\n",
    "    'NTB Sales',\n",
    "    'NTB Units'])\n",
    "\n",
    "\n",
    "    return cleaned_file\n",
    "\n",
    "#Finally I export the file to csv, as I found that the difference in time was massive!\n",
    "# CSV takes max 2 seconds, while xlsx takes up to 30-40 seconds.\n",
    "\n",
    "Sponsored_Brand_ASIN(SBASIN).to_csv(r\"C:file-destination.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
