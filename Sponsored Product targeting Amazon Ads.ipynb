{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Data Cleaning project\n",
    "\n",
    "\n",
    "This Jupyter notebook is part of an automation project for data cleaning in Amazon Ads. Managing two distinct ad accounts, each with its unique set of brands, campaign types, and targetings, poses significant challenges in data processing and management. Initially, the data cleaning process was cumbersome and inefficient, primarily conducted in Excel, which proved to be unreliable and time-consuming.\n",
    "\n",
    "The primary objective of this project is to streamline the data cleaning process, reducing the steps involved to a minimum and significantly cutting down the time required. This notebook demonstrates a sample of the data cleaning procedure applied to various sheets, which previously took 1-2 hours, but can now be completed in a matter of seconds.\n",
    "\n",
    "Along the way I've created multiple functions that will come in handy in my future projects!\n",
    "\n",
    "Future enhancements include the integration of Watchdog, an automation tool, to further simplify the process. The goal is to achieve a system where new files are automatically detected and processed, producing the final output without manual intervention.\n",
    "\n",
    "This code, along with its detailed documentation, will be shared on my GitHub account, offering insights into the methods and reasoning behind this automated data cleaning approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries for the data cleaning process\n",
    "\n",
    "import pandas as pd\n",
    "import unicodedata #  The unicodedata module is imported to handle Arabic characters effectively.\n",
    "import re #used to extract the number of the file\n",
    "import sys\n",
    "import os\n",
    "import concurrent.futures # for more efficient execution of importing files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\openpyxl\\styles\\stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    }
   ],
   "source": [
    "# Path to the directory with your files\n",
    "path_main_file = 'main file' # platform results was only available for 90 days, which is why I've created a main file that contains all the consolidated data to add to it  \n",
    "Path_new_file = 'new_file_path' # path for the new file\n",
    "products_file_path = 'Product_file' # products are coded, So I imported a file with the product name - code - brand for better analysis\n",
    "\n",
    "def read_excel_file(file_path, columns=None):\n",
    "    \"\"\"This function is to make importing xlsx more robust\"\"\"\n",
    "    if columns:\n",
    "        return pd.read_excel(file_path, usecols=columns)\n",
    "    else:\n",
    "        return pd.read_excel(file_path)\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    SPST_new_file,products_file_main = executor.map(read_excel_file,[Path_new_file,products_file_path])\n",
    "\n",
    "\n",
    "SPST_main = path_main_file # I added a new variable so that I don't need to re-import the files again\n",
    "SPASIN_main_file = pd.read_csv(path_main_file)\n",
    "SPASIN_main_file['Date'] = pd.to_datetime(SPASIN_main_file['Date'])\n",
    "\n",
    "SPST = SPST_new_file # I added a new variable so that I don't need to re-import the files again\n",
    "SPST['source_file'] = os.path.basename(Path_new_file)\n",
    "SPST['source_file_n'] = SPST['source_file'].apply(lambda x: re.search(r'\\((\\d+)\\)', x).group(1) if re.search(r'\\((\\d+)\\)', x) else None)\n",
    "SPST['source_file_number'] = SPST['source_file_n'].astype(int)\n",
    "SPST['Date'] = pd.to_datetime(SPST['Date'])\n",
    "\n",
    "products_file = products_file_main # I added a new variable so that I don't need to re-import the files again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sponsored_Product_SPST(SPST,SPASIN_main_file):\n",
    "    \"\"\"This script processes and merges the newly imported data with the primary dataset.\"\"\"\n",
    "    # formula to get the type of targeting\n",
    "    def determine_targeting(SPST):\n",
    "\n",
    "        \"\"\"We had 4 different targeting types, this function is designed to differentiate between them\n",
    "            and it's specific campaign\"\"\"\n",
    "        if SPST['Match Type'] == '-' and 'category' in SPST['Targeting']:\n",
    "            return 'Category Targeting'\n",
    "            \n",
    "        elif SPST['Match Type'] == '-' and 'asin' in SPST['Targeting']:\n",
    "            return 'ASIN Targeting'\n",
    "        \n",
    "        elif SPST['Match Type'] != '-':\n",
    "            return 'Keyword Targeting'\n",
    "\n",
    "        else:\n",
    "            return 'Automatic Targeting'\n",
    "\n",
    "    # formula to get the specific (Keyword - Category - ASIN) we're targeting\n",
    "    def Keyword_category_asin(SPST):\n",
    "            \"\"\"Within each targeting type we needed to identify the specific targeting\n",
    "                ex.the followng is an example for the report category=\"Powdered Spices & Seasonings\"\n",
    "                 we needed to export the category within (Powdered Spices & Seasonings)\n",
    "                for ASINs it was the same thing: asin=\"B09V51WF73\", this was needed to be able\n",
    "                 to identify the ASIN and its brand for better understanding of performance. \"\"\"\n",
    "            try:\n",
    "                if SPST['Match Type'] == '-':\n",
    "                    start_pos = SPST['Targeting'].find('\"')+1\n",
    "                    end_pos = SPST['Targeting'].rfind('\"')-1\n",
    "                    if start_pos > 0 and end_pos > start_pos:\n",
    "                        # Extract substring from one position after '=' to the end\n",
    "                        return SPST['Targeting'][start_pos:end_pos]\n",
    "                    else:\n",
    "                        # if '=' is not found, return the full 'Targeting' string\n",
    "                        return SPST['Targeting']\n",
    "                else:\n",
    "                    # if '-' isn't found, return full 'Targeting' string\n",
    "                    return SPST['Targeting']\n",
    "            \n",
    "            except:\n",
    "                # if there is an error return the 'Targeting' string\n",
    "                return 'Automatic Targeting'\n",
    "\n",
    "    # Adding columns with the clearly identified targeting and targeting types.\n",
    "    SPST['Targeting Type'] = SPST.apply(determine_targeting, axis=1)\n",
    "    SPST['Keyword_category_asin'] = SPST.apply(Keyword_category_asin, axis=1)\n",
    "\n",
    "\n",
    "    # removing unicodedata and converting to float\n",
    "    def clean_convert_to_numeric(column):\n",
    "        \"\"\"Because Arabic letters are RTL this posed an interesting challenge as even after removed it left a space that couldn't be deleted, therefore i needed to deal with the RTL characters and remove them\"\"\"\n",
    "\n",
    "        def remove_rtl_characters(text):\n",
    "            \"\"\"Removing RTL characters to process the data correctly\"\"\"\n",
    "            return ''.join(char for char in text if unicodedata.category(char) != 'Lm' and unicodedata.category(char) != 'Cf')\n",
    "        \n",
    "        cleaned_column = column.astype(str).apply(remove_rtl_characters)\n",
    "        cleaned_column = cleaned_column.str.replace('ج.م.', '', regex=False)\n",
    "        return pd.to_numeric(cleaned_column, errors='coerce')\n",
    "\n",
    "    SPST['14 Day Total Sales'] = clean_convert_to_numeric(SPST['14 Day Total Sales'])\n",
    "    SPST['Spend'] = clean_convert_to_numeric(SPST['Spend']) \n",
    "    products_file['ASIN'] = products_file['ASIN'].str.lower()\n",
    "    \n",
    "\n",
    "    # merging with the product file to get a better understanding of which ASINs we appear on\n",
    "    SPST = SPST.merge(products_file,how='left',left_on='Customer Search Term', right_on='ASIN')\n",
    "    SPST['Brand Description'].fillna('', inplace=True)\n",
    "    \n",
    "    # remove rows with 0 impressions\n",
    "    cleaned_file = SPST[SPST['Impressions'] != 0]\n",
    "\n",
    "    # Renaming columns \n",
    "    cleaned_file = cleaned_file.rename(columns={'14 Day Total Sales': 'Total Sales', '14 Day Total Orders (#)': 'Total Orders', '14 Day Total Units (#)': 'Total Units','14 Day Advertised ASIN Units (#)':'Total Units Advertised ASINS','14 Day Brand Halo ASIN Units (#)':'Total Units Brand Halo ASIN', '14 Day Advertised ASIN Sales':'Total Sales Advertised ASINS','14 Day Brand Halo ASIN Sales':'Total Sales Brand Halo ASIN'})\n",
    "    \n",
    "    # labeling branded and non branded based on keyword - search term - asin\n",
    "    def Branded_NotBranded(row):\n",
    "        \"\"\"This helped me identify the weight of spending as well as our sales for branded and non-branded targeting\"\"\"\n",
    "        keyword = str(row['Keyword_category_asin']).lower()  # Convert to string and lower case\n",
    "        search_term = str(row['Customer Search Term']).lower()  # Convert to string and lower case\n",
    "        brand_description = str(row['Brand Description']).lower()\n",
    "        \n",
    "        # List of example brands in lowercase\n",
    "        brands = ['juhayna', 'molto', 'rameda', 'masrawy', 'biscomisr', 'seoudi', 'zahran', 'baraka', 'banque misr', 'cleopatra', 'edfa3ly', 'el araby', 'fresh', 'goldi', 'mezza luna', \"temmy's\"]\n",
    "\n",
    "        # checking if our brand is mentioned either in search term or in targeting, this helps with deeper investigation to check our performance branded-nonbranded\n",
    "        \n",
    "        if any(brand in keyword for brand in brands):\n",
    "                return 'Branded'\n",
    "        \n",
    "        if any(brand in search_term for brand in brands):\n",
    "                return \"Branded search term\"\n",
    "\n",
    "        # check if ASIN product isn't null, meaning that the customer landed on our product\n",
    "        if  any(brand in brand_description for brand in brands):\n",
    "                return 'Branded searched ASIN'\n",
    "\n",
    "        return 'Not Branded'\n",
    "\n",
    "\n",
    "    # Adding the branded_nonbranded column    \n",
    "    cleaned_file['Branded_NotBranded'] = cleaned_file.apply(Branded_NotBranded, axis=1)\n",
    "\n",
    "\n",
    "    # sort by date\n",
    "    cleaned_file.sort_values(by=['Date'], inplace=True)\n",
    "\n",
    "    #fillna\n",
    "    cleaned_file.fillna('', inplace=True)\n",
    "\n",
    "    # We follow a naming system for each campaign and we have to write the brand we're advertising to\n",
    "    # The following is to export the brand name.\n",
    "    cleaned_file['Brand'] = cleaned_file['Campaign Name'].str.split('_').str[0]\n",
    "\n",
    "    #day of week\n",
    "    cleaned_file['Day of week'] = cleaned_file['Date'].dt.dayofweek\n",
    "    # map the day of the week number to its name\n",
    "    cleaned_file['Day of week'] = cleaned_file['Day of week'].map({\n",
    "        0: 'Monday',\n",
    "        1: 'Tuesday',\n",
    "        2: 'Wednesday',\n",
    "        3: 'Thursday',\n",
    "        4: 'Friday',\n",
    "        5: 'Saturday',\n",
    "        6: 'Sunday'\n",
    "    })\n",
    "\n",
    "    #Re-arranging columns and exporting only important columns\n",
    "    cleaned_file = cleaned_file.reindex(columns=['Date',\n",
    "    'Day of week',\n",
    "    'Campaign Name',\n",
    "    'Ad Group Name',\n",
    "    'Brand',\n",
    "    'Targeting Type',\n",
    "    'Keyword_category_asin',\n",
    "    'Branded_NotBranded',\n",
    "    'Match Type',\n",
    "    'Customer Search Term',\n",
    "    'Impressions',\n",
    "    'Clicks',\n",
    "    'Spend',\n",
    "    'Total Sales',\n",
    "    'Total Orders',\n",
    "    'Total Units',\n",
    "    'Total Units Advertised ASINS',\n",
    "    'Total Sales Advertised ASINS',    \n",
    "    'Total Units Brand Halo ASIN',\n",
    "    'Total Sales Brand Halo ASIN',\n",
    "    'source_file_number'])\n",
    "\n",
    "\n",
    "    # After cleaning the new dataset (most recent 90 days), we concatenate it with the main dataset(for entire ad account history)\n",
    "    # remove any duplication so only the most updated results remain\n",
    "\n",
    "    combined_df = pd.concat([cleaned_file,SPASIN_main_file])\n",
    "    combined_df.sort_values(by=['source_file_number','Date','Campaign Name','Ad Group Name','Keyword_category_asin','Match Type','Customer Search Term'], ascending=[False,True,True,True,True,True,True], inplace=True)\n",
    "    #remove duplicated\n",
    "    \n",
    "    combined_df.drop_duplicates(subset=['Date','Campaign Name','Ad Group Name','Keyword_category_asin','Match Type','Customer Search Term'], keep='first', inplace=True)\n",
    "\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# it's more efficient to export in csv.\n",
    "\n",
    "Sponsored_Product_SPST(SPST,SPASIN_main_file).to_csv(r\"C:\\Work\\\\Unilever\\ECOM\\Base Reports\\Final automated reports\\Sponsored_Product_SPST_Unilever.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
